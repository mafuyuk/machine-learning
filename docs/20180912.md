# GCP講座
数値変数をバケツにいれるとカテゴリー変数として扱える
年齢を 10代20代とか

dataflowは気の利いたhadoopみたいなもの


モデルに使用するデータとテストに使用するデータを7:3で分けることが多い
分け方はランダムではなくハッシュ値がよい

理由:
2000年以降のデータを使ってモデル生成をしていたが、１年経って2001年以降がモデル生成対象になった際にハッシュ値だと前回も今回もテストに使われるデータは同じだから


tensorflow
コードは書きやすいがtf.kerasは分散学習にまだちゃんと対応していない。tf.learnは分散学習ができる

Wideモデルがカテゴリ型データ
Deepモデルが数値型データ
を使う


# ワークフロー
### ログ収集
Big Queryにデータを貯め込む

### 解析
NoteBookからBigQueryのデータを使って、データの傾向を解析
使えるデータやどういうモデルを作るか考える

### 前処理
使えるデータをBigQueryから抜き出しtrainingとtestに分割したり
カテゴリ系のunknownデータ水増しが必要な場合はそれをCloud Dataflowで行いCSV形式にしてCloud Strageに吐き出す

### モデルを作る
NotebookでCloud Storageにあるcsvの学習データを使って、Pythonのコードを作るところ

### モデルを学習させる 
Managed ML ServiceからCloud Storageにバイナリデータが吐き出される部分

### モデルを利用できるようにする
Managed ML Serviceでバイナリを読み込みAPIとして提供